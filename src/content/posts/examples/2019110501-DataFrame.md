---
title: PySpark|DataFrameæ“ä½œæŒ‡å—
published: 2024-12-19
tags: ["PySpark","Spark"]
lang: zh
abbrlink: dataframe
---

æœ€è¿‘æœ‰ç”¨åˆ°PySparkåŠä¸€äº›DataFrameä¹‹ç±»çš„æ“ä½œï¼Œç®€å•åšä¸ªæ•´ç†ã€‚

# æŸ¥

## è¡Œå…ƒç´ æŸ¥è¯¢æ“ä½œ

åƒSQLé‚£æ ·æ‰“å°åˆ—è¡¨å‰20å…ƒç´ 
showå‡½æ•°å†…å¯ç”¨intç±»åž‹æŒ‡å®šè¦æ‰“å°çš„è¡Œæ•°ï¼š

```python
df.show()
df.show(30)
```

æŸ¥è¯¢æ¦‚å†µ,èŽ·å–æŒ‡å®šå­—æ®µçš„ç»Ÿè®¡ä¿¡æ¯`describe(cols: String*)`

è¿™ä¸ªæ–¹æ³•å¯ä»¥åŠ¨æ€çš„ä¼ å…¥ä¸€ä¸ªæˆ–å¤šä¸ªStringç±»åž‹çš„å­—æ®µåï¼Œç»“æžœä»ç„¶ä¸ºDataFrameå¯¹è±¡ï¼Œç”¨äºŽç»Ÿè®¡æ•°å€¼ç±»åž‹å­—æ®µçš„ç»Ÿè®¡å€¼ï¼Œæ¯”å¦‚count, mean, stddev, min, maxç­‰ã€‚
ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼Œå…¶ä¸­c1å­—æ®µä¸ºå­—ç¬¦ç±»åž‹ï¼Œc2å­—æ®µä¸ºæ•´åž‹ï¼Œc4å­—æ®µä¸ºæµ®ç‚¹åž‹

```python
jdbcDF .describe("c1" , "c2", "c4" ).show()
```

ä»¥åŠæŸ¥è¯¢ç±»åž‹ï¼Œä¹‹å‰æ˜¯`type`ï¼ŒçŽ°åœ¨æ˜¯`df.printSchema()`ä»¥æ ‘çš„å½¢å¼æ‰“å°æ¦‚è¦ã€‚

```
root
 |-- user_pin: string (nullable = true)
 |-- a: string (nullable = true)
 |-- b: string (nullable = true)
 |-- c: string (nullable = true)
 |-- d: string (nullable = true)
 |-- e: string (nullable = true)
```

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œåªæ˜¯æ‰“å°å‡ºæ¥ã€‚

`first, head, take, takeAsListï¼š`èŽ·å–è‹¥å¹²è¡Œè®°å½•
è¿™é‡Œåˆ—å‡ºçš„å››ä¸ªæ–¹æ³•æ¯”è¾ƒç±»ä¼¼ï¼Œå…¶ä¸­

- `first`èŽ·å–ç¬¬ä¸€è¡Œè®°å½•
- `head`èŽ·å–ç¬¬ä¸€è¡Œè®°å½•ï¼Œ`head(n: Int)`èŽ·å–å‰nè¡Œè®°å½•
- `take(n: Int)`èŽ·å–å‰nè¡Œæ•°æ®
- `takeAsList(n: Int)`èŽ·å–å‰nè¡Œæ•°æ®ï¼Œå¹¶ä»¥`List`çš„å½¢å¼å±•çŽ°ï¼Œä»¥Rowæˆ–è€…Array[Row]çš„å½¢å¼è¿”å›žä¸€è¡Œæˆ–å¤šè¡Œæ•°æ®ã€‚

`first`å’Œ`head`åŠŸèƒ½ç›¸åŒã€‚

`take`å’Œ`takeAsList`æ–¹æ³•ä¼šå°†èŽ·å¾—åˆ°çš„æ•°æ®è¿”å›žåˆ°`Driver`ç«¯ï¼Œæ‰€ä»¥ï¼Œä½¿ç”¨è¿™ä¸¤ä¸ªæ–¹æ³•æ—¶éœ€è¦æ³¨æ„æ•°æ®é‡ï¼Œä»¥å…`Driver`å‘ç”Ÿ`OutOfMemoryError`

èŽ·å–å¤´å‡ è¡Œåˆ°æœ¬åœ°ï¼š

```python
ist = df.head(3)   # Example: [Row(a=1, b=1), Row(a=2, b=2), ... ...]
list = df.take(5)   # Example: [Row(a=1, b=1), Row(a=2, b=2), ... ...]
```

æŸ¥è¯¢æ€»è¡Œæ•°ï¼š

```python
 int_num = df.count()
```

å–åˆ«å

```python
df.select(df.age.alias('age_value'),'name')
```

æŸ¥è¯¢æŸåˆ—ä¸ºnullçš„è¡Œï¼š

```python
from pyspark.sql.functions import isnull
df = df.filter(isnull("col_a"))
```

è¾“å‡º`list`ç±»åž‹ï¼Œ`list`ä¸­æ¯ä¸ªå…ƒç´ æ˜¯Rowç±»ï¼š

```python
list = df.collect()
```

æ³¨ï¼šæ­¤æ–¹æ³•å°†æ‰€æœ‰æ•°æ®å…¨éƒ¨å¯¼å…¥åˆ°æœ¬åœ°ï¼Œè¿”å›žä¸€ä¸ªArrayå¯¹è±¡

åŽ»é‡setæ“ä½œ

```python
data.select('columns').distinct().show()
```

è·Ÿ`py`ä¸­çš„setä¸€æ ·ï¼Œå¯ä»¥`distinct()`ä¸€ä¸‹åŽ»é‡ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥`.count()`è®¡ç®—å‰©ä½™ä¸ªæ•°

éšæœºæŠ½æ ·
éšæœºæŠ½æ ·æœ‰ä¸¤ç§æ–¹å¼ï¼Œä¸€ç§æ˜¯åœ¨HIVEé‡Œé¢æŸ¥æ•°éšæœºï¼›å¦ä¸€ç§æ˜¯åœ¨`pyspark`ä¹‹ä¸­ã€‚

`HIVE`é‡Œé¢æŸ¥æ•°éšæœº

```sql
sql = "select * from data order by rand()  limit 2000"
```

`pyspark`ä¹‹ä¸­

```python
sample = result.sample(False,0.5,0) # randomly select 50% of lines
```

## åˆ—å…ƒç´ æ“ä½œ

èŽ·å–Rowå…ƒç´ çš„æ‰€æœ‰åˆ—åï¼š

```python
r = Row(age=11, name='Alice')
print r.columns    #  ['age', 'name']
```

é€‰æ‹©ä¸€åˆ—æˆ–å¤šåˆ—ï¼š`select`

```python
df["age"]
df.age
df.select(â€œnameâ€)
df.select(df[â€˜nameâ€™], df[â€˜ageâ€™]+1)
df.select(df.a, df.b, df.c)    # é€‰æ‹©aã€bã€cä¸‰åˆ—
df.select(df["a"], df["b"], df["c"])    # é€‰æ‹©aã€bã€cä¸‰åˆ—
```

é‡è½½çš„`select`æ–¹æ³•ï¼š

```python
jdbcDF.select(jdbcDF( "id" ), jdbcDF( "id") + 1 ).show( false)
```

ä¼šåŒæ—¶æ˜¾ç¤º`idåˆ— + id + 1åˆ—`

è¿˜å¯ä»¥ç”¨`where`æŒ‰æ¡ä»¶é€‰æ‹©

```python
jdbcDF .where("id = 1 or c1 = 'b'" ).show()
```

## æŽ’åº

`orderBy`å’Œ`sort`ï¼šæŒ‰æŒ‡å®šå­—æ®µæŽ’åºï¼Œé»˜è®¤ä¸ºå‡åº

```python
train.orderBy(train.Purchase.desc()).show(5)
```

```
Output:
+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+
|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|
+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+
|1003160| P00052842|     M|26-35|        17|            C|                         3|             0|                10|                15|              null|   23961|
|1002272| P00052842|     M|26-35|         0|            C|                         1|             0|                10|                15|              null|   23961|
|1001474| P00052842|     M|26-35|         4|            A|                         2|             1|                10|                15|              null|   23961|
|1005848| P00119342|     M|51-55|        20|            A|                         0|             1|                10|                13|              null|   23960|
|1005596| P00117642|     M|36-45|        12|            B|                         1|             0|                10|                16|              null|   23960|
+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+
only showing top 5 rows
```

æŒ‰æŒ‡å®šå­—æ®µæŽ’åºã€‚åŠ ä¸ª`-`è¡¨ç¤ºé™åºæŽ’åº

## æŠ½æ ·

`sample`æ˜¯æŠ½æ ·å‡½æ•°

```python
t1 = train.sample(False, 0.2, 42)
t2 = train.sample(False, 0.2, 43)
t1.count(),t2.count()
```

```
Output:
(109812, 109745)
```

`withReplacement = True or False`ä»£è¡¨æ˜¯å¦æœ‰æ”¾å›žã€‚
`fraction = x, where x = .5ï¼Œ`ä»£è¡¨æŠ½å–ç™¾åˆ†æ¯”

## æŒ‰æ¡ä»¶ç­›é€‰when / between

`when(condition, value1).otherwise(value2)`è”åˆä½¿ç”¨ï¼š
é‚£ä¹ˆï¼šå½“æ»¡è¶³æ¡ä»¶`condition`çš„æŒ‡èµ‹å€¼ä¸º`values1`,ä¸æ»¡è¶³æ¡ä»¶çš„åˆ™èµ‹å€¼ä¸º`values2`.
`otherwise`è¡¨ç¤ºï¼Œä¸æ»¡è¶³æ¡ä»¶çš„æƒ…å†µä¸‹ï¼Œåº”è¯¥èµ‹å€¼ä¸ºå•¥ã€‚

`demo1`

```python
from pyspark.sql import functions as F
df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()
```

```
+-----+------------------------------------------------------------+
| name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|
+-----+------------------------------------------------------------+
|Alice|                                                          -1|
|  Bob|                                                           1|
+-----+------------------------------------------------------------+
```

`demo 2:å¤šä¸ªwhenä¸²è”`

```python
df = df.withColumn('mod_val_test1',F.when(df['rand'] <= 0.35,1).when(df['rand'] <= 0.7, 2).otherwise(3))
between(lowerBound, upperBound)
ç­›é€‰å‡ºæŸä¸ªèŒƒå›´å†…çš„å€¼ï¼Œè¿”å›žçš„æ˜¯TRUE or FALSE
```

```python
df.select(df.name, df.age.between(2, 4)).show()
```

```
+-----+---------------------------+
| name|((age >= 2) AND (age <= 4))|
+-----+---------------------------+
|Alice|                       true|
|  Bob|                      false|
+-----+---------------------------+
```

# å¢žã€æ”¹

## æ–°å»ºæ•°æ®

æœ‰è¿™ä¹ˆä¸¤ç§å¸¸è§„çš„æ–°å»ºæ•°æ®æ–¹å¼ï¼š`createDataFrame`ã€`.toDF()`

```python
sqlContext.createDataFrame(pd.dataframe())
```

æ˜¯æŠŠ`pandas`çš„`dataframe`è½¬åŒ–ä¸º`spark.dataframe`æ ¼å¼ï¼Œæ‰€ä»¥å¯ä»¥ä½œä¸ºä¸¤è€…çš„æ ¼å¼è½¬åŒ–

```python
from pyspark.sql import Row
row = Row("spe_id", "InOther")
x = ['x1','x2']
y = ['y1','y2']
new_df = sc.parallelize([row(x[i], y[i]) for i in range(2)]).toDF()
```

`Row`ä»£è¡¨çš„æ˜¯è¯¥æ•°æ®é›†çš„åˆ—åã€‚

## æ–°å¢žæ•°æ®åˆ— withColumn

`withColumn`æ˜¯é€šè¿‡æ·»åŠ æˆ–æ›¿æ¢ä¸ŽçŽ°æœ‰åˆ—æœ‰ç›¸åŒçš„åå­—çš„åˆ—ï¼Œè¿”å›žä¸€ä¸ªæ–°çš„`DataFrame`

```python
result3.withColumn('label', 0)
```

æˆ–è€…æ¡ˆä¾‹

```python
train.withColumn('Purchase_new',train.Purchase/2.0).select('Purchase','Purchase_new').show(5)
```

```
Output:
+--------+------------+
|Purchase|Purchase_new|
+--------+------------+
|    8370|      4185.0|
|   15200|      7600.0|
|    1422|       711.0|
|    1057|       528.5|
|    7969|      3984.5|
+--------+------------+
only showing top 5 rows
```

**æŠ¥é”™ï¼š**`AssertionError: col should be Column`ï¼Œä¸€å®šè¦æŒ‡å®šæŸçŽ°æœ‰åˆ—

æœ‰ä¸¤ç§æ–¹å¼å¯ä»¥å®žçŽ°ï¼š

ä¸€ç§æ–¹å¼é€šè¿‡`functions`

```python
from pyspark.sql import functions
result3 = result3.withColumn('label',  functions.lit(0))
```

ä½†æ˜¯å¦‚ä½•æ–°å¢žä¸€ä¸ªç‰¹åˆ«`List`
`python`ä¸­çš„`list`ä¸èƒ½ç›´æŽ¥æ·»åŠ åˆ°`dataframe`ä¸­ï¼Œéœ€è¦å…ˆå°†`list`è½¬ä¸ºæ–°çš„`dataframe`,ç„¶åŽæ–°çš„`dataframe`å’Œè€çš„`dataframe`è¿›è¡Œ`join`æ“ä½œ, ä¸‹é¢çš„ä¾‹å­ä¼šå…ˆæ–°å»ºä¸€ä¸ª`dataframe`ï¼Œç„¶åŽå°†`list`è½¬ä¸º`dataframe`ï¼Œç„¶åŽå°†ä¸¤è€…`join`èµ·æ¥ã€‚

```python
from pyspark.sql.functions import lit

df = sqlContext.createDataFrame(
    [(1, "a", 23.0), (3, "B", -23.0)], ("x1", "x2", "x3"))
from pyspark.sql.functions import monotonically_increasing_id
df = df.withColumn("id", monotonically_increasing_id())
df.show()
```

```
+---+---+-----+---+
| x1| x2|   x3| id|
+---+---+-----+---+
|  1|  a| 23.0|  0|
|  3|  B|-23.0|  1|
+---+---+-----+---+
```

```python
from pyspark.sql import Row
l = ['jerry', 'tom']
row = Row("pid", "name")
new_df = sc.parallelize([row(i, l[i]) for i in range(0,len(l))]).toDF()
new_df.show()

```

```
+---+-----+
|pid| name|
+---+-----+
|  0|jerry|
|  1|  tom|
+---+-----+
```

```python
join_df = df.join(new_df, df.id==new_df.pid)
join_df.show()
```

```
+---+---+-----+---+---+-----+
| x1| x2|   x3| id|pid| name|
+---+---+-----+---+---+-----+
|  1|  a| 23.0|  0|  0|jerry|
|  3|  B|-23.0|  1|  1|  tom|
+---+---+-----+---+---+-----+
```

**æ³¨æ„ï¼ï¼ï¼**å…¶ä¸­ï¼Œ`monotonically_increasing_id()`ç”Ÿæˆçš„IDä¿è¯æ˜¯å•è°ƒé€’å¢žå’Œå”¯ä¸€çš„ï¼Œä½†ä¸æ˜¯è¿žç»­çš„ã€‚
æ‰€ä»¥ï¼Œæœ‰å¯èƒ½ï¼Œå•è°ƒåˆ°1-140000ï¼Œåˆ°äº†ç¬¬144848ä¸ªï¼Œå°±å˜æˆä¸€é•¿ä¸²ï¼š8845648744563ï¼Œæ‰€ä»¥åƒä¸‡è¦æ³¨æ„ï¼ï¼

å¦ä¸€ç§æ–¹å¼é€šè¿‡å¦ä¸€ä¸ªå·²æœ‰å˜é‡ï¼š

```python
result3 = result3.withColumn('label',  df.result*0 )
```

ä¿®æ”¹åŽŸæœ‰`df[â€œxxâ€]`åˆ—çš„æ‰€æœ‰å€¼ï¼š

```python
df = df.withColumn(â€œxxâ€, 1)
```

ä¿®æ”¹åˆ—çš„ç±»åž‹ï¼ˆç±»åž‹æŠ•å°„ï¼‰ï¼š

```python
df = df.withColumn("year2", df["year1"].cast("Int"))
```

ä¿®æ”¹åˆ—å:

```python
jdbcDF.withColumnRenamed( "id" , "idx" )
```

## è¿‡æ»¤æ•°æ®

è¿‡æ»¤æ•°æ®ï¼ˆ`filter`å’Œ`where`æ–¹æ³•ç›¸åŒï¼‰ï¼š

```python
df = df.filter(df['age']>21)
df = df.where(df['age']>21)
#å¤šä¸ªæ¡ä»¶
jdbcDF .filter(â€œid = 1 or c1 = â€˜bâ€™â€ ).show()
```

å¯¹`null`æˆ–`nan`æ•°æ®è¿›è¡Œè¿‡æ»¤ï¼š

```python
from pyspark.sql.functions import isnan, isnull
df = df.filter(isnull("a"))  # æŠŠaåˆ—é‡Œé¢æ•°æ®ä¸ºnullçš„ç­›é€‰å‡ºæ¥ï¼ˆä»£è¡¨pythonçš„Noneç±»åž‹ï¼‰
df = df.filter(isnan("a"))  # æŠŠaåˆ—é‡Œé¢æ•°æ®ä¸ºnançš„ç­›é€‰å‡ºæ¥ï¼ˆNot a Numberï¼Œéžæ•°å­—æ•°æ®ï¼‰
```

# åˆå¹¶ join / union

## æ¨ªå‘æ‹¼æŽ¥

```python
result3 = result1.union(result2)
jdbcDF.unionALL(jdbcDF.limit(1)) # unionALL
```

## Joinæ ¹æ®æ¡ä»¶

**å•å­—æ®µJoin**
åˆå¹¶2ä¸ªè¡¨çš„`join`æ–¹æ³•ï¼š

```python
df_join = df_left.join(df_right, df_left.key == df_right.key, "inner")
```

å…¶ä¸­ï¼Œæ–¹æ³•å¯ä»¥ä¸ºï¼š`inner, outer, left_outer, right_outer, leftsemi.`
å…¶ä¸­æ³¨æ„ï¼Œä¸€èˆ¬éœ€è¦æ”¹ä¸ºï¼š`left_outer`

**å¤šå­—æ®µjoin**

```python
joinDF1.join(joinDF2, Seq("id", "name")ï¼‰
```

**æ··åˆå­—æ®µ**

```python
joinDF1.join(joinDF2 , joinDF1("id" ) === joinDF2( "t1_id"))
```

è·Ÿpandas é‡Œé¢çš„`left_on,right_on`

## æ±‚å¹¶é›†ã€äº¤é›†

æ¥çœ‹ä¸€ä¸ªä¾‹å­ï¼Œå…ˆæž„é€ ä¸¤ä¸ª`dataframe`ï¼š

```python
sentenceDataFrame = spark.createDataFrame((
      (1, "asf"),
      (2, "2143"),
      (3, "rfds")
    )).toDF("label", "sentence")
sentenceDataFrame.show()

sentenceDataFrame1 = spark.createDataFrame((
      (1, "asf"),
      (2, "2143"),
      (4, "f8934y")
    )).toDF("label", "sentence")
```

## å·®é›†

```python
newDF = sentenceDataFrame1.select("sentence").subtract(sentenceDataFrame.select("sentence"))
newDF.show()
```

```
+--------+
|sentence|
+--------+
|  f8934y|
+--------+
```

## äº¤é›†

```python
newDF = sentenceDataFrame1.select("sentence").intersect(sentenceDataFrame.select("sentence"))
newDF.show()
```

```
+--------+
|sentence|
+--------+
|     asf|
|    2143|
+--------+
```

## å¹¶é›†

### union

```python
newDF = sentenceDataFrame1.select("sentence").union(sentenceDataFrame.select("sentence"))
newDF.show()
```

```
+--------+
|sentence|
+--------+
|     asf|
|    2143|
|  f8934y|
|     asf|
|    2143|
|    rfds|
+--------+
```

### unionAll

`unionAll`æ–¹æ³•ï¼šå¯¹ä¸¤ä¸ªDataFrameè¿›è¡Œç»„åˆ,ç±»ä¼¼äºŽ`SQL`ä¸­çš„`UNION ALL`æ“ä½œã€‚

```python
jdbcDF.unionALL(jdbcDF.limit(1))
```

## å¹¶é›† + åŽ»é‡

```python
newDF = sentenceDataFrame1.select("sentence").union(sentenceDataFrame.select("sentence")).distinct()
newDF.show()
```

```
+--------+
|sentence|
+--------+
|    rfds|
|     asf|
|    2143|
|  f8934y|
+--------+
```

## åˆ†å‰²ï¼šè¡Œè½¬åˆ—

æœ‰æ—¶å€™éœ€è¦æ ¹æ®æŸä¸ªå­—æ®µå†…å®¹è¿›è¡Œåˆ†å‰²ï¼Œç„¶åŽç”Ÿæˆå¤šè¡Œï¼Œè¿™æ—¶å¯ä»¥ä½¿ç”¨`explode`æ–¹æ³•
ä¸‹é¢ä»£ç ä¸­ï¼Œæ ¹æ®`c3`å­—æ®µä¸­çš„ç©ºæ ¼å°†å­—æ®µå†…å®¹è¿›è¡Œåˆ†å‰²ï¼Œåˆ†å‰²çš„å†…å®¹å­˜å‚¨åœ¨æ–°çš„å­—æ®µ`c3_`ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤º

```python
jdbcDF.explode( "c3" , "c3_" ){time: String => time.split( " " )}
```

# ç»Ÿè®¡

## é¢‘æ•°ç»Ÿè®¡ä¸Žç­›é€‰

```python
jdbcDF.stat.freqItems(Seq ("c1") , 0.3).show()
```

æ ¹æ®`c4`å­—æ®µï¼Œç»Ÿè®¡è¯¥å­—æ®µå€¼å‡ºçŽ°é¢‘çŽ‡åœ¨30%ä»¥ä¸Šçš„å†…å®¹

## åˆ†ç»„ç»Ÿè®¡

**äº¤å‰åˆ†æž**

```python
train.crosstab('Age', 'Gender').show()
```

```
Output:
+----------+-----+------+
|Age_Gender|    F|     M|
+----------+-----+------+
|      0-17| 5083| 10019|
|     46-50|13199| 32502|
|     18-25|24628| 75032|
|     36-45|27170| 82843|
|       55+| 5083| 16421|
|     51-55| 9894| 28607|
|     26-35|50752|168835|
+----------+-----+------+
```

**groupByæ–¹æ³•æ•´åˆï¼š**

```python
train.groupby('Age').agg({'Purchase': 'mean'}).show()
```

```
Output:
+-----+-----------------+
|  Age|    avg(Purchase)|
+-----+-----------------+
|51-55|9534.808030960236|
|46-50|9208.625697468327|
| 0-17|8933.464640444974|
|36-45|9331.350694917874|
|26-35|9252.690632869888|
|  55+|9336.280459449405|
|18-25|9169.663606261289|
+-----+-----------------+
```

**å¦å¤–ä¸€äº›demoï¼š**

```python
df['x1'].groupby(df['x2']).count().reset_index(name='x1')
```

**åˆ†ç»„æ±‡æ€»**

```python
train.groupby('Age').count().show()
```

```
Output:
+-----+------+
|  Age| count|
+-----+------+
|51-55| 38501|
|46-50| 45701|
| 0-17| 15102|
|36-45|110013|
|26-35|219587|
|  55+| 21504|
|18-25| 99660|
+-----+------+
```

**åº”ç”¨å¤šä¸ªå‡½æ•°ï¼š**

```python
from pyspark.sql import functions
df.groupBy(â€œAâ€).agg(functions.avg(â€œBâ€), functions.min(â€œBâ€), functions.max(â€œBâ€)).show()
```

æ•´åˆåŽ`GroupedData`ç±»åž‹å¯ç”¨çš„æ–¹æ³•ï¼ˆå‡è¿”å›ž`DataFrame`ç±»åž‹ï¼‰ï¼š
**avg(cols)** â€”â€”   è®¡ç®—æ¯ç»„ä¸­ä¸€åˆ—æˆ–å¤šåˆ—çš„å¹³å‡å€¼
**count()** â€”â€”   è®¡ç®—æ¯ç»„ä¸­ä¸€å…±æœ‰å¤šå°‘è¡Œï¼Œè¿”å›ž`DataFrame`æœ‰2åˆ—ï¼Œä¸€åˆ—ä¸ºåˆ†ç»„çš„ç»„åï¼Œå¦ä¸€åˆ—ä¸ºè¡Œæ€»æ•°
**max(cols)** â€”â€”   è®¡ç®—æ¯ç»„ä¸­ä¸€åˆ—æˆ–å¤šåˆ—çš„æœ€å¤§å€¼
**mean(cols)** â€”â€”  è®¡ç®—æ¯ç»„ä¸­ä¸€åˆ—æˆ–å¤šåˆ—çš„å¹³å‡å€¼
**min(cols)** â€”â€”  è®¡ç®—æ¯ç»„ä¸­ä¸€åˆ—æˆ–å¤šåˆ—çš„æœ€å°å€¼
**sum(cols)** â€”â€”   è®¡ç®—æ¯ç»„ä¸­ä¸€åˆ—æˆ–å¤šåˆ—çš„æ€»å’Œ

## apply å‡½æ•°

å°†`df`çš„æ¯ä¸€åˆ—åº”ç”¨å‡½æ•°fï¼š

```python
df.foreach(f) æˆ–è€… df.rdd.foreach(f)
```

å°†`df`çš„æ¯ä¸€å—åº”ç”¨å‡½æ•°fï¼š

å°†dfçš„æ¯ä¸€å—åº”ç”¨å‡½æ•°fï¼š

```
df.foreachPartition(f) æˆ–è€… df.rdd.foreachPartition(f)
```

## ã€Mapå’ŒReduceåº”ç”¨ã€‘è¿”å›žç±»åž‹seqRDDs

### mapå‡½æ•°åº”ç”¨

å¯ä»¥å‚è€ƒï¼š[Spark Python API](https://spark.apache.org/docs/latest/api/python/index.html ) å­¦ä¹ 

```python
train.select('User_ID').rdd.map(lambda x:(x,1)).take(5)
```

```
Output:
[(Row(User_ID=1000001), 1),
 (Row(User_ID=1000001), 1),
 (Row(User_ID=1000001), 1),
 (Row(User_ID=1000001), 1),
 (Row(User_ID=1000002), 1)]
```

å…¶ä¸­mapåœ¨spark2.0å°±ç§»é™¤äº†ï¼Œæ‰€ä»¥åªèƒ½ç”±`rdd.`è°ƒç”¨ã€‚

```python
data.select('col').rdd.map(lambda l: 1 if l in ['a','b'] else 0 ).collect()

print(x.collect())
print(y.collect())
```

```
[1, 2, 3]
[(1, 1), (2, 4), (3, 9)]
```

è¿˜æœ‰ä¸€ç§æ–¹å¼`mapPartitions`ï¼š

```python
def _map_to_pandas(rdds):
    return [pd.DataFrame(list(rdds))]

data.rdd.mapPartitions(_map_to_pandas).collect()
```

è¿”å›žçš„æ˜¯`list`ã€‚

### udf å‡½æ•°åº”ç”¨

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
import datetime
```

**å®šä¹‰ä¸€ä¸ª udf å‡½æ•°**

```python
def today(day):
    if day==None:
        return datetime.datetime.fromtimestamp(int(time.time())).strftime('%Y-%m-%d')
    else:
        return day

#è¿”å›žç±»åž‹ä¸ºå­—ç¬¦ä¸²ç±»åž‹
udfday = udf(today, StringType())
```

**ä½¿ç”¨**

```python
df.withColumn('day', udfday(df.day))
```

æœ‰ç‚¹ç±»ä¼¼`apply`,å®šä¹‰ä¸€ä¸ª `udf` æ–¹æ³•, ç”¨æ¥è¿”å›žä»Šå¤©çš„æ—¥æœŸ`(yyyy-MM-dd)`

## åˆ é™¤

```python
df.drop('age').collect()
df.drop(df.age).collect()
```

`dropna`å‡½æ•°ï¼š

```python
df = df.na.drop()  # æ‰”æŽ‰ä»»ä½•åˆ—åŒ…å«naçš„è¡Œ
df = df.dropna(subset=['col_name1', 'col_name2'])  # æ‰”æŽ‰col1æˆ–col2ä¸­ä»»ä¸€ä¸€åˆ—åŒ…å«naçš„è¡Œ
```

```python
train.dropna().count()
```

```
Output:
166821
```

å¡«å……`NA`åŒ…æ‹¬`fillna`

```python
train.fillna(-1).show(2)
```

```
Output:
+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+
|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|
+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+
|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|                -1|                -1|    8370|
|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|
+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+
only showing top 2 rows
```

## åŽ»é‡

### distinctï¼šè¿”å›žä¸€ä¸ªä¸åŒ…å«é‡å¤è®°å½•çš„DataFrame

è¿”å›žå½“å‰`DataFrame`ä¸­ä¸é‡å¤çš„Rowè®°å½•ã€‚è¯¥æ–¹æ³•å’ŒæŽ¥ä¸‹æ¥çš„`dropDuplicates()`æ–¹æ³•ä¸ä¼ å…¥æŒ‡å®šå­—æ®µæ—¶çš„ç»“æžœç›¸åŒã€‚

```python
jdbcDF.distinct()
```

### dropDuplicatesï¼šæ ¹æ®æŒ‡å®šå­—æ®µåŽ»é‡

æ ¹æ®æŒ‡å®šå­—æ®µåŽ»é‡ã€‚ç±»ä¼¼äºŽ`select distinct a, b`æ“ä½œ
ç¤ºä¾‹ï¼š

```python
train.select('Age','Gender').dropDuplicates().show()
```

```
Output:
+-----+------+
|  Age|Gender|
+-----+------+
|51-55|     F|
|51-55|     M|
|26-35|     F|
|26-35|     M|
|36-45|     F|
|36-45|     M|
|46-50|     F|
|46-50|     M|
|  55+|     F|
|  55+|     M|
|18-25|     F|
| 0-17|     F|
|18-25|     M|
| 0-17|     M|
+-----+------+
```

### åŽ»é™¤ä¸¤ä¸ªè¡¨é‡å¤çš„å†…å®¹

åœºæ™¯æ˜¯è¦ï¼Œä¾æ®Bè¡¨ä¸ŽAè¡¨å…±æœ‰çš„å†…å®¹ï¼Œéœ€è¦åŽ»é™¤è¿™éƒ¨åˆ†å…±æœ‰çš„ã€‚
ä½¿ç”¨çš„é€»è¾‘æ˜¯åˆå¹¶ä¸¤å¼ è¡¨ï¼Œç„¶åŽæŠŠåŒ¹é…åˆ°çš„åˆ é™¤å³å¯ã€‚

```python
from pyspark.sql import functions
def LeftDeleteRight(test_left,test_right,left_col = 'user_pin',right_col = 'user_pin'):
    print('right data process ...')
    columns_right = test_right.columns
    test_right = test_right.withColumn('user_pin_right', test_right[right_col])
    test_right = test_right.withColumn('notDelete',  functions.lit(0))
    # åˆ é™¤å…¶ä½™çš„
    for col in columns_right:
        test_right = test_right.drop(col)
    # åˆå¹¶
    print('rbind left and right data ...')
    test_left = test_left.join(test_right, test_left[left_col] == test_right['user_pin_right'], "left")
    test_left = test_left.fillna(1)
    test_left = test_left.where('notDelete =1')
    # åŽ»æŽ‰å¤šä½™çš„å­—æ®µ
    for col in ['user_pin_right','notDelete']:
        test_left = test_left.drop(col)
    return test_left

test_left = LeftDeleteRight(test_b,test_a,left_col = 'user_pin',right_col = 'user_pin')
```

# æ ¼å¼è½¬æ¢

`Pandas`å’Œ`spark.dataframe`äº’è½¬
`Pandas`å’Œ`Spark`çš„`DataFrame`ä¸¤è€…äº’ç›¸è½¬æ¢ï¼š

```python
pandas_df = spark_df.toPandas()
spark_df = sqlContext.createDataFrame(pandas_df)
```

è½¬åŒ–ä¸ºpandasï¼Œä½†æ˜¯è¯¥æ•°æ®è¦è¯»å…¥å†…å­˜ï¼Œå¦‚æžœæ•°æ®é‡å¤§çš„è¯ï¼Œå¾ˆéš¾è·‘å¾—åŠ¨

ä¸¤è€…çš„å¼‚åŒï¼š

Pyspark DataFrameæ˜¯åœ¨åˆ†å¸ƒå¼èŠ‚ç‚¹ä¸Šè¿è¡Œä¸€äº›æ•°æ®æ“ä½œï¼Œè€Œpandasæ˜¯ä¸å¯èƒ½çš„ï¼›
Pyspark DataFrameçš„æ•°æ®åæ˜ æ¯”è¾ƒç¼“æ…¢ï¼Œæ²¡æœ‰Pandasé‚£ä¹ˆåŠæ—¶åæ˜ ï¼›
Pyspark DataFrameçš„æ•°æ®æ¡†æ˜¯ä¸å¯å˜çš„ï¼Œä¸èƒ½ä»»æ„æ·»åŠ åˆ—ï¼Œåªèƒ½é€šè¿‡åˆå¹¶è¿›è¡Œï¼›
pandasæ¯”Pyspark DataFrameæœ‰æ›´å¤šæ–¹ä¾¿çš„æ“ä½œä»¥åŠå¾ˆå¼ºå¤§

**è½¬åŒ–ä¸ºRDD**
ä¸ŽSpark RDDçš„ç›¸äº’è½¬æ¢ï¼š

```python
rdd_df = df.rdd
df = rdd_df.toDF()
```

# SQLæ“ä½œ

`DataFrame`æ³¨å†Œæˆ`SQL`çš„è¡¨ï¼š

```python
df.createOrReplaceTempView("TBL1")
```

è¿›è¡Œ`SQL`æŸ¥è¯¢ï¼ˆè¿”å›ž`DataFrame`ï¼‰ï¼š

```python
conf = SparkConf()
ss = SparkSession.builder.appName("APP_NAME").config(conf=conf).getOrCreate()

df = ss.sql(â€œSELECT name, age FROM TBL1 WHERE age >= 13 AND age <= 19â€³)
```

# è¯»å†™csv

åœ¨Pythonä¸­ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨`SQLContext`ç±»ä¸­ `load/save`å‡½æ•°æ¥è¯»å–å’Œä¿å­˜`CSV`æ–‡ä»¶ï¼š

```python
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
df = sqlContext.load(source="com.databricks.spark.csv", header="true", path = "cars.csv")
df.select("year", "model").save("newcars.csv", "com.databricks.spark.csv",header="true")
```

å…¶ä¸­ï¼Œ`header`ä»£è¡¨æ˜¯å¦æ˜¾ç¤ºè¡¨å¤´ã€‚
å…¶ä¸­ä¸»å‡½æ•°ï¼š

```python
save(path=None, format=None, mode=None, partitionBy=None, **options)[source]
```

```
Parameters:

path â€“ the path in a Hadoop supported file system

format â€“ the format used to save

mode â€“

specifies the behavior of the save operation when data already
exists.

append: Append contents of this DataFrame to existing data.

overwrite: Overwrite existing data.

ignore: Silently ignore this operation if data already exists.

error (default case): Throw an exception if data already exists.

partitionBy â€“ names of partitioning columns

options â€“ all other string options
```

ðŸ“–[å‚è€ƒ](https://blog.csdn.net/sinat_26917383/article/details/80500349)

------
